In this chapter Python and the module NumPy is introduced. The first
part of the chapter briefly describes Python and some of its more
important structures. The second part describes some important
structures that NumPy introduces, as well as a description of some
basic NumPy functionality and memory layout.

NumPy has replaced Numeric as the standard way of implementing
multidimensional arrays in Python. NumPy implements the necessary
functionality for supporting and manipulating the ndarrays. NumPy has
support for \BLAS{} operations as well as other common operations. It
supports use of external libraries, so e.g. an external \BLAS{}
library could be used instead of the \BLAS{} operations that comes
standard with NumPy.

%In this chapter Python is introduced. First Python is briefly covered,
%why do we need Python, what is its limitations and how can it be
%optimized. Second, Python can be extended with C extensions, thus a
%small walk through of how to do this is presented. Next is a coverage
%of the module NumPy, how it organized and how its structures are build
%and used. Lastly, a \CBE\ enhanced structure for NumPy is presented.


%Numeric was used as data structure for the scientific module SciPy,
%but now NumPy is. NumPy features several operations for organizing and
%modifying the data structure, but it also introduces \BLAS{} and
%\LAPACK{}(all of it???).

\section{Why Python}

Python is a high level programming language. It has a very intuitive
syntax, which gives the programmer an easy way of representing data
and easy control. Python has existed since the end of the 80's and has
accumulated a large code base. Python has been used, not only for
simple scripting, but to a large extend also for scientific
applications. As a consequence of the large code base and intuitive
syntax, many scientists use Python to solve their problems.

Python is developed using the C-programming language. It can be
thought of as having two states. A pure Python state and a C
state. For example, when iterating through an array, it can be done in
pure Python or it can be done in C code. When iterating in pure
Python, the generated byte code handles the iteration, but when
iterating in C, it can be done using compiled machine code. An example
of this could be if the elements of an array should be summed. In pure
Python the ``for value in range'' statement would be called, summing
the values for each element in the range. The C solution would lead to
the call ``range.sum'', which would execute some underlying binary C
code, thus resulting in better performance.

This exemplifies the real performance problem with Python. The code is not
compiled to binary machine code, but byte code, and then run on the Python
virtual machine. This gives great portability, but slows down
execution.

Therefore, when real compute intensive problems needs to be solved, it is better to
develop the solution in C, rather than pure Python, and then use Python as
a link between the C program and the scientist. This has been done extensively through
the lifespan of Python.

This solution to the performance problem gives rise to a new problem. The
reason why many choose Python, is because it is simple and intuitive to use,
not because people are interested in programming C as well. For this reason
Python includes a rich library of C and Fortran programs for all
kinds of situations. Furthermore, Python is easily extended using these
two programming languages\footnote{There are more branches of Python
using other programming languages such as Java.}.\\

%In the following sections some relevant aspects of Python are described,
%leading up to an introduction of NumPy and how a datastructure for NumPy
%should be designed for running efficiently on the \CBE{}.

%In the following sections Python is ripped apart, to see what it
%requires to extend it a new \CBE\ structure for NumPy.


\section{Python Objects}

In Python everything is objects. Every object is composed of the
\texttt{PyObject} structure. For example, the int object looks like
this:

\examplecode{code/intst.c}{Int structure}{lst:intst}

%\begin{verbatim}
%typedef struct {
%    PyObject_HEAD
%    long ob_ival;
%} PyIntObject;
%\end{verbatim}

The \texttt{HEAD} part of the object includes macros needed for all
objects. Including reference counting and to enable special debugging
fields.

Every Python module needs a \texttt{PyTypeObject} object, see example
\ref{lst:pytypeobject} for a small sample. The type object gives
various information about the object. Including the size of the
structure, its name, what member functions are implemented and used,
how to access member variables, customized comparisons between
objects, how to print the object and much more. For each member
function a module has, it must be declared in its
\texttt{PyMethodDef} object. Each function is declared with a
function name, C-function, number of arguments and a
description. Furthermore, a \texttt{PyMemberDef} object can expose the
member variables as attributes. If a given member should be visible,
it should be included in this object. For member variables it can be
convenient to declare get- and set-functions. These must be provided
in the \texttt{PyGetSetDef} object. For each variable the name of it
the get and set function names must be defined.

\examplecode{code/pytypeobject.c}{A small sample of the PyTypeObject structure}{lst:pytypeobject}

\subsection{Creation and Initialization of Objects}

To create a Python object a \texttt{new} function must be
provided. This function calls the allocation function\footnote{This
function allocates memory to the object. A generic function is
provided for implementation ease.} and should initialize the member
variables to appropriate values. If all the member variables are to be
initialized as NULL, a generic initialization helper function is
provided. If any of the members should not be NULL, additional code
should be provided to initialize the members appropriately.

Additionally, an initialization function can be provided. This
function can take arguments to give the members default values. The
\function{new} function cannot explicitly be called, but the initialization
function can. Thus special care must be taken when working with member
objects, because when overwriting old objects, their reference counts
must consequently be decreased.

These two functions must be specified in the \texttt{PyTypeObject}
object, in the \textit{tp\_new} and \textit{tp\_init} slots.

\subsection{Object Members}

As stated before, every exposed member variable and member functions
in the object, must be declared in \texttt{PyMemberDef} and
\texttt{PyMethodDef}, respectively. Both objects must be specified in the
\texttt{PyTypeObject} object, \textit{tp\_members} and
\textit{tp\_methods}.

Each member function, defined in the \texttt{PyMethodDef}, must be
implemented by the programmer.

The get and set functions object, \texttt{PyGetSetDef}, must be
specified in the \texttt{PyTypeObject} object in the
\textit{tp\_getseters} slot. The variables that are declared in
\texttt{PyGetSetDef}, must not be in the \texttt{PyMemberDef} object,
because, if the variable is an object, the programmer has no control
over what kinds of objects are sat.

%\subsection{Garbage Collection}

%Do we need this??? Its not that interesting!

\subsection{Installing Modules}

When the module is finished, it must be setup to be used in
Python. This is done by a \file{setup.py} script. It uses
distutils\footnote{As of Python 2.x distutils is used to install
modules} to setup the module. Basically it should provide module name,
version and extension modules(in form of source code files). Distutils
handles the installation and compilation.

\section{The NumPy Structure}
\label{sec:python-numpy}

NumPy is primarily two things, a homogeneous data structure(the
ndarray) and a universal function object(UFUNC). \\

The NumPy structure builds upon, as all extension for Python does, the
few structures mentioned in the previous section. While the int object
only had one item, other than the head, the NumPy \struct{PyArrayObject} structure
includes a lot more. This can be seen in listing \ref{lst:npy_st}.

\examplecode{code/numpystruct.c}{NumPy structure}{lst:npy_st}

The \textit{PyArray\_Desc} contains information about the specific
type at hand. A more detailed examination of the structure is found
later in this section and the structure can be seen in
listing \ref{lst:npy_dtype}.

\examplecode{code/pyarray_desc_st.c}{NumPy data-type(dtype) descriptor structure}{lst:npy_dtype}

\examplecode{code/pyarrayinterface.c}{NumPy data structure C interface}{lst:npy_c_interface}

NumPy includes many mathematical operations. The universal function
object is developed, so it is able to perform all operations on all
types of ndarrays. Every function is an instance of this object and
each of them performs element by element operations on one or multiple
ndarrays. The structure can be seen in listing \ref{lst:npy_ufunc_st}.

\examplecode{code/pyufuncobject.c}{NumPy universal function object structure}{lst:npy_ufunc_st}

%\subsection{Memory Layout}

%The NumPy ndarray is stored in memory in a contiguous manner. The
%layout can be thought of as a 1-dimensional array. The indexes of
%multi-dimensional arrays are mapped into this 1d array. No padding is
%added to the 1d layout in memory. JMI: This is not final! Still work
%in progress. The layout does not need to be contiguous it seems (with
%respect to the entire array). See NumPy guide etc.

%\section{Proposed CBE Enhanced Structure}

%In this section we will propose a structure to replace the current
%NumPy structure. As it was stated in the previous section, the current
%NumPy structure is one dimensional, thus giving problems on the
%\CBE\ architecture. We will discuss it in detail, why it is a better
%structure than the current and where potential problems lies.

%\subsection{Two Dimensional Data}

%For easy \DMA\ transfers on the \CBE\ architecture, the data must be
%16 byte aligned, thus 32 bit values must be grouped in four for easy
%transfers. If the total number of values is divisible by four, extra
%data must be appended or else the transfer will cause a bus error.

%In Numpy the data structure is one dimensional, as stated earlier,
%leaving \DMA\ transfers complicated, because if a matrix should be
%distributed among several \SPE{}s, for example, the number of data
%chunks should be divisible by four and that is not always the case.

%This leads to an idea of a two dimensional data structure, where, no
%matter what, the data can always be divisible by four and hence
%trivial \DMA\ transfers can be done.

%See section X for a further description and analysis of data structure requirements
%and proposed solution.

%\subsection{Meta Data}

\section{PPE or SPE}

NumPy has a great deal of functionality. Some of it is maintenance
functionality, i. e. ensuring the lengths of two operands are the
same. NumPy also includes a lot of computational functionality. It is
sensible to let the \SPE{}'s do all the computational work and let
the \PPE\ do the maintenance.

With this separation it is clear where each of the NumPy
functionalities should be put and it follows the idea behind the \PPE\
/ \SPE\ relationship.

\section{Performance on the Cell BE}
\label{sec:performanceoncell}

When wanting to parallelize NumPy, it is good to have its
functionality analyzed. We will group its functionality into two
categories before moving on:

\begin{itemize}
\item{Simple functionality}
\item{Composite functionality}
\end{itemize}

When thinking of functionality, it is important to define
functionality. We define one functionality as a single operation in
NumPy. That is, for example, a call to an operation that multiplies
two arrays or solves a linear equation. Both are one functionality,
but they each constitutes one of the categories.

We will discuss and analyze the two categories. Find their possible
pitfalls and their strengths. This will give us an idea of how to
implement NumPy and what to expect of its performance.


\subsection{Simple Functionality}
\label{sec:simple_functionality}

A simple functionality is basically one single operation. This could
be an addition of two arrays or taking the square root of every
element in one array. Most of NumPy's simple core functionality is
computed element wise. For example, the addition is defined as

\[
c_i = a_i + b_i
\]

and square root as

\[
b_i = \sqrt{a_i}
\]

This leads to the potential biggest problem with element wise simple
functionality, they are not computational bound. They do one operation
per element in the array for unary functions and one operation for
every other element in the array for binary functions.

As discussed in section \ref{sec:transferworkload} on
page \pageref{sec:transferworkload}, this is a typical example of the
transfer / workload ratio problem. There is not enough data for
the \SPU\ to work with, so it finishes its computations before the
next transfer is done. Because of this, it is not possible to achieve
linear speed up with simple functionality.

\subsection{Composite Functionality}
\label{sec:composite_functionality}

Unlike simple functionality, composite functionality is build up of
multiple operations. This could be a NumPy function exclusively made
from C, but it could also be made from Python or a mix. For example
the function \function{solve()}, which solves the linear equation

\[
Ax = b
\]

where $A$ is a two dimensional array and $b$ is either one or two
dimensional.


This kind of functionality has a bigger potential to be computational
bound, which gives a better transfer / workload ratio. This means,
that the \SPU{}'s will be kept busy more and the performance of the
parallelization will converge to optimum. Where optimum is linear
speed up, based on a single \SPE\ executing the operation.\\

The simple and composite functionality, is analogous to level 1 \BLAS\
routines and level 3 \BLAS\ routines.

%Though it seems, that 
